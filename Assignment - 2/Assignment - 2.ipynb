{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4677c383",
   "metadata": {},
   "source": [
    "# ASSIGNMENT - 2 - SAIKIRAN PULLABHATLA - A8MOH5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c103597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "from pyquaternion import Quaternion\n",
    "import numpy as np\n",
    "from nuscenes.utils.geometry_utils import points_in_box\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ccd1639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 1.069 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# Initialize NuScenes\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='C:/Users/saiki/3D Sensing and Sensor Fusion/Assignemnt - 2/v1.0-mini', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02415c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to apply rotation and translation transformations\n",
    "def apply_transformation(points, trans_vector, rot_quaternion):\n",
    "    \"\"\"\n",
    "    Transform a set of 3D points from one coordinate frame to another.\n",
    "    Args:\n",
    "        points: Array of points (N, 3) in the original coordinate system.\n",
    "        trans_vector: Translation vector (3,).\n",
    "        rot_quaternion: Rotation quaternion.\n",
    "    Returns:\n",
    "        Transformed points in the new coordinate frame.\n",
    "    \"\"\"\n",
    "   \n",
    "     # Apply transpose and rotation and translation\n",
    "    transformed = rot_quaternion.rotation_matrix @ points.T \n",
    "    transformed += trans_vector.reshape(3, 1)  \n",
    "    return transformed.T  \n",
    "\n",
    "\n",
    "# Function to filter out points belonging to moving objects\n",
    "def remove_moving_objects(points, nusc, lidar_data):\n",
    "    \"\"\"\n",
    "    Filter LiDAR points that belong to dynamic objects like vehicles, bicycles, etc.\n",
    "    Args:\n",
    "        points: Array of points (N, 3) in global coordinates.\n",
    "        nusc: NuScenes dataset instance.\n",
    "        lidar_data: Lidar sample data dictionary.\n",
    "    Returns:\n",
    "        Static points (points not belonging to dynamic objects).\n",
    "    \"\"\"\n",
    "     # Initialize mask: all points are initially \"static\"\n",
    "    mask = np.ones(points.shape[0], dtype=bool) \n",
    "    \n",
    "    # Retrieve annotations for the sample\n",
    "    sample_data = nusc.get('sample', lidar_data['sample_token'])  \n",
    "    for ann_token in sample_data['anns']:\n",
    "        annotation = nusc.get('sample_annotation', ann_token)\n",
    "        \n",
    "        # Check if annotation corresponds to dynamic objects \n",
    "        if annotation['category_name'].startswith(('vehicle', 'bicycle', 'motorcycle')):\n",
    "            \n",
    "            # Get the bounding box of the object\n",
    "            box = nusc.get_box(annotation['token']) \n",
    "            \n",
    "            # Exclude points inside the bounding box\n",
    "            mask &= ~points_in_box(box, points.T)  \n",
    "    return points[mask]  \n",
    "\n",
    "\n",
    "\n",
    "# Function to colorize points based on camera images\n",
    "def colorize_points(nusc, global_points, ego_pose, lidar_data):\n",
    "    \"\"\"\n",
    "    Project LiDAR points onto camera images and assign RGB values to them.\n",
    "    Args:\n",
    "        nusc: NuScenes dataset instance.\n",
    "        global_points: Array of points (N, 3) in global coordinates.\n",
    "        ego_pose: Vehicle's ego pose data.\n",
    "        lidar_data: Lidar sample data dictionary.\n",
    "    Returns:\n",
    "        Colors: RGB values for each point.\n",
    "        Mask: Boolean mask indicating which points have valid color values.\n",
    "    \"\"\"\n",
    "    cameras = ['CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_FRONT_LEFT']\n",
    "    colors = np.zeros((global_points.shape[0], 3))  # Initialize color array\n",
    "    mask = np.zeros(global_points.shape[0], dtype=bool)  # Initialize mask: all points are uncolored initially\n",
    "\n",
    "    for cam in cameras:  # Loop through all six cameras\n",
    "        try:\n",
    "            # Load the camera image and calibration data\n",
    "            cam_data = nusc.get('sample_data', nusc.get('sample', lidar_data['sample_token'])['data'][cam])\n",
    "            image = np.array(Image.open(nusc.get_sample_data_path(cam_data['token'])))\n",
    "            cam_calib = nusc.get('calibrated_sensor', cam_data['calibrated_sensor_token'])\n",
    "\n",
    "            # Camera extrinsic parameters\n",
    "            cam_rot = Quaternion(cam_calib['rotation'])\n",
    "            cam_trans = np.array(cam_calib['translation'])\n",
    "\n",
    "            # Vehicle pose\n",
    "            ego_rot = Quaternion(ego_pose['rotation'])\n",
    "            ego_trans = np.array(ego_pose['translation'])\n",
    "\n",
    "            # Transform points from global frame to camera frame\n",
    "            cam_global_rot = ego_rot * cam_rot\n",
    "            cam_global_trans = ego_trans + ego_rot.rotation_matrix @ cam_trans\n",
    "            points_cam = (global_points - cam_global_trans) @ cam_global_rot.rotation_matrix\n",
    "\n",
    "            # Project points onto the camera image\n",
    "            intrinsic = np.array(cam_calib['camera_intrinsic'])\n",
    "            proj_points = (intrinsic @ points_cam.T).T\n",
    "            depths = points_cam[:, 2]\n",
    "\n",
    "            # Mask points that are in front of the camera\n",
    "            front_mask = depths > 0\n",
    "            proj_points = proj_points[front_mask]\n",
    "            proj_points[:, :2] /= proj_points[:, 2:3]\n",
    "\n",
    "            # Mask points that fall inside the image dimensions\n",
    "            in_image = (proj_points[:, 0] >= 0) & (proj_points[:, 0] < image.shape[1]) & \\\n",
    "                       (proj_points[:, 1] >= 0) & (proj_points[:, 1] < image.shape[0])\n",
    "\n",
    "            for idx, proj in zip(np.where(front_mask)[0][in_image], proj_points[in_image]):\n",
    "                x, y = int(proj[0]), int(proj[1])\n",
    "                colors[idx] = image[y, x] / 255.0  # Assign RGB color to the point\n",
    "                mask[idx] = True\n",
    "        except:\n",
    "            continue\n",
    "    return colors, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c47a1",
   "metadata": {},
   "source": [
    "# STEP 1: Aggregated Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d96577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing Aggregated Point Clouds...\n",
      "Visualization Aggregated Point Clouds done\n"
     ]
    }
   ],
   "source": [
    "scene = nusc.scene[0]\n",
    "lidar_token = nusc.get('sample', scene['first_sample_token'])['data']['LIDAR_TOP']\n",
    "\n",
    "clouds_raw = []\n",
    "while lidar_token:\n",
    "    # Load LiDAR data\n",
    "    lidar_data = nusc.get('sample_data', lidar_token)\n",
    "    lidar_points = LidarPointCloud.from_file(nusc.get_sample_data_path(lidar_token)).points[:3, :].T\n",
    "\n",
    "    # Transform LiDAR points to the global coordinate system\n",
    "    calib = nusc.get('calibrated_sensor', lidar_data['calibrated_sensor_token'])\n",
    "    ego_pose = nusc.get('ego_pose', lidar_data['ego_pose_token'])\n",
    "    vehicle_points = apply_transformation(lidar_points, np.array(calib['translation']), Quaternion(calib['rotation']))\n",
    "    global_points = apply_transformation(vehicle_points, np.array(ego_pose['translation']), Quaternion(ego_pose['rotation']))\n",
    "\n",
    "    # Create and store the point cloud\n",
    "    cloud = o3d.geometry.PointCloud()\n",
    "    cloud.points = o3d.utility.Vector3dVector(global_points)\n",
    "    clouds_raw.append(cloud)\n",
    "    lidar_token = lidar_data['next']\n",
    "\n",
    "print(\"Visualizing Aggregated Point Clouds...\")\n",
    "o3d.visualization.draw_geometries(clouds_raw, window_name=\"Aggregated Point Clouds\")\n",
    "\n",
    "print(\"Visualization Aggregated Point Clouds done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db262068",
   "metadata": {},
   "source": [
    "# Step 2: Moving Object Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4dbeffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing Moving Object Filtered Point Clouds...\n",
      "Visualization Moving Object Filtered Point Clouds is done\n"
     ]
    }
   ],
   "source": [
    "filtered_clouds = []\n",
    "for cloud in clouds_raw:\n",
    "    static_points = remove_moving_objects(np.asarray(cloud.points), nusc, lidar_data)\n",
    "    filtered_cloud = o3d.geometry.PointCloud()\n",
    "    filtered_cloud.points = o3d.utility.Vector3dVector(static_points)\n",
    "    filtered_cloud.paint_uniform_color([1, 0, 0])  # Red for static points\n",
    "    filtered_clouds.append(filtered_cloud)\n",
    "\n",
    "print(\"Visualizing Moving Object Filtered Point Clouds...\")\n",
    "o3d.visualization.draw_geometries(filtered_clouds, window_name=\"Moving Object Filtered Point Clouds\")\n",
    "\n",
    "print(\"Visualization Moving Object Filtered Point Clouds is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805ad46",
   "metadata": {},
   "source": [
    "# Step 3: Colorization of Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d76757a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing Colorized Point Clouds...\n",
      "Visualization Colorized Point Clouds is done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "colorized_clouds = []\n",
    "lidar_token = nusc.get('sample', scene['first_sample_token'])['data']['LIDAR_TOP']\n",
    "\n",
    "for cloud in clouds_raw:\n",
    "    lidar_data = nusc.get('sample_data', lidar_token)\n",
    "    ego_pose = nusc.get('ego_pose', lidar_data['ego_pose_token'])\n",
    "    colors, mask = colorize_points(nusc, np.asarray(cloud.points), ego_pose, lidar_data)\n",
    "\n",
    "    # Apply colors to valid points\n",
    "    colorized_cloud = o3d.geometry.PointCloud()\n",
    "    colorized_cloud.points = o3d.utility.Vector3dVector(np.asarray(cloud.points)[mask])\n",
    "    colorized_cloud.colors = o3d.utility.Vector3dVector(colors[mask])\n",
    "    colorized_clouds.append(colorized_cloud)\n",
    "    lidar_token = lidar_data['next']\n",
    "\n",
    "print(\"Visualizing Colorized Point Clouds...\")\n",
    "o3d.visualization.draw_geometries(colorized_clouds, window_name=\"Colorized Point Clouds\")\n",
    "\n",
    "print(\"Visualization Colorized Point Clouds is done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
